{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "35cbf193-8759-4fef-8448-3466bfb378ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\n",
    "\n",
    "spark = SparkSession.builder.appName('Spam detection').getOrCreate()\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "428ec211-9537-404a-ad46-a86d9ead281b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               email|label|\n",
      "+--------------------+-----+\n",
      "| date wed NUMBER ...|    0|\n",
      "|martin a posted t...|    0|\n",
      "|man threatens exp...|    0|\n",
      "|klez the virus th...|    0|\n",
      "| in adding cream ...|    0|\n",
      "| i just had to ju...|    0|\n",
      "|the scotsman NUMB...|    0|\n",
      "|martin adamson wr...|    0|\n",
      "|the scotsman thu ...|    0|\n",
      "|i have been tryin...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# baca file\n",
    "theFile = spark.read.csv('spam_email.csv', inferSchema=True, header=True)\n",
    "theFile.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "38ce64fb-95a1-4881-99e4-0f9ee5f76dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               email|label|\n",
      "+--------------------+-----+\n",
      "| save up to NUMBE...|    1|\n",
      "|NUMBER fight the ...|    1|\n",
      "|NUMBER fight the ...|    1|\n",
      "| adult club offer...|    1|\n",
      "|i thought you mig...|    1|\n",
      "|a powerhouse gift...|    1|\n",
      "|help wanted we ar...|    1|\n",
      "| hyperlink life c...|    1|\n",
      "|tired of the bull...|    1|\n",
      "|dear ricardoNUMBE...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filterData = theFile.filter((theFile['label'] != 0))\n",
    "filterData.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f058822d-7c81-4044-bbda-b6b6a8453dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Data Training : 2106\n",
      "Jumlah Data Testing : 894\n",
      "+--------------------+-----+--------------------+\n",
      "|               email|label|           kataPecah|\n",
      "+--------------------+-----+--------------------+\n",
      "| NUMBER NUMBER NU...|    1|[, number, number...|\n",
      "| NUMBER NUMBER an...|    1|[, number, number...|\n",
      "| NUMBER NUMBER ho...|    1|[, number, number...|\n",
      "| NUMBER hits here...|    0|[, number, hits, ...|\n",
      "| NUMBER in the ma...|    0|[, number, in, th...|\n",
      "| NUMBER minutes i...|    0|[, number, minute...|\n",
      "|                URL |    0|             [, url]|\n",
      "| URL act dtl open...|    0|[, url, act, dtl,...|\n",
      "| URL additional c...|    0|[, url, additiona...|\n",
      "| URL additional c...|    0|[, url, additiona...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# latih data\n",
    "bagiData = theFile.randomSplit([0.7, 0.3])\n",
    "Trdata = bagiData[0] # data latih\n",
    "TeData = bagiData[1] # data testing\n",
    "\n",
    "print(f'Jumlah Data Training : {Trdata.count()}\\nJumlah Data Testing : {TeData.count()}')\n",
    "\n",
    "# pecah kata\n",
    "tokenizer = Tokenizer(inputCol=\"email\", outputCol=\"kataPecah\")\n",
    "tokenizerLatih = tokenizer.transform(Trdata)\n",
    "tokenizerLatih.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "91b4e31a-a011-4bde-b222-3ba404079c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|               email|label|           kataPecah|        Kata Berarti|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "| NUMBER NUMBER NU...|    1|[, number, number...|[, number, number...|\n",
      "| NUMBER NUMBER an...|    1|[, number, number...|[, number, number...|\n",
      "| NUMBER NUMBER ho...|    1|[, number, number...|[, number, number...|\n",
      "| NUMBER hits here...|    0|[, number, hits, ...|[, number, hits, ...|\n",
      "| NUMBER in the ma...|    0|[, number, in, th...|[, number, main, ...|\n",
      "| NUMBER minutes i...|    0|[, number, minute...|[, number, minute...|\n",
      "|                URL |    0|             [, url]|             [, url]|\n",
      "| URL act dtl open...|    0|[, url, act, dtl,...|[, url, act, dtl,...|\n",
      "| URL additional c...|    0|[, url, additiona...|[, url, additiona...|\n",
      "| URL additional c...|    0|[, url, additiona...|[, url, additiona...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopWord = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"Kata Berarti\")\n",
    "stopWordLatih = stopWord.transform(tokenizerLatih)\n",
    "stopWordLatih.show(truncate=True, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2590f798-9f4e-4231-8fdf-efc1a022922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|           KataPecah|        Kata Berarti|               Hasil|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|    1|[, number, number...|[, number, number...|(262144,[21622,13...|\n",
      "|    1|[, number, number...|[, number, number...|(262144,[4475,140...|\n",
      "|    1|[, number, number...|[, number, number...|(262144,[535,4214...|\n",
      "|    0|[, number, hits, ...|[, number, hits, ...|(262144,[14376,21...|\n",
      "|    0|[, number, in, th...|[, number, main, ...|(262144,[8618,978...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# menghapus kata menjadi numerik\n",
    "hashingTF = HashingTF(inputCol=stopWord.getOutputCol(), outputCol='Hasil')\n",
    "dataAngka = hashingTF.transform(stopWordLatih).select(\n",
    "    'label', 'KataPecah', 'Kata Berarti', 'Hasil'\n",
    ")\n",
    "dataAngka.show(truncate=True, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0556d054-f63c-485e-be13-403cd30519bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is Done\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"Hasil\", maxIter=10, regParam=0.01)\n",
    "model = lr.fit(dataAngka)\n",
    "print(\"Training is Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cbd4999d-4353-46b2-9ba8-76682e693717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1, Kata Berarti=['stumbling', 'greatest', 'way', 'marketing', 'century', 'undoubtedly', 'direct', 'e', 'mail', 'similar', 'postman', 'delivering', 'letter', 'mailbox', 'ability', 'promote', 'product', 'service', 'website', 'mlm', 'network', 'marketing', 'opportunity', 'millions', 'instantly', 'advertisers', 'dreaming', 'number', 'years', 'e', 'mail', 'one', 'page', 'promotion', 'list', 'general', 'addresses', 'greatest', 'part', 'completely', 'affordable', 'e', 'mail', 'marketing', 'answer', 'know', 'know', 'exactly', 'proven', 'fact', 'attract', 'new', 'business', 'direct', 'e', 'mail', 'marketing', 'profits', 'e', 'mail', 'advertising', 'generate', 'amazing', 'living', 'proof', 'direct', 'e', 'mail', 'internet', 'advertising', 'company', 'clients', 'pay', 'us', 'thousands', 'dollars', 'week', 'e', 'mail', 'products', 'services', 'want', 'one', 'spending', 'thousands', 'direct', 'email', 'marketing', 'campane', 'testing', 'market', 'see', 'works', 'standard', 'pricing', 'procedures', 'extracting', 'list', 'general', 'internet', 'addreses', 'actually', 'extracted', 'popular', 'web', 'sites', 'internet', 'addresses', 'verified', 'run', 'purification', 'process', 'process', 'includes', 'addresses', 'run', 'custom', 'remove', 'filter', 'number', 'number', 'keywords', 'well', 'numbermb', 'remove', 'flamer', 'list', 'edu', 'org', 'gov', 'mil', 'us', 'domains', 'removed', 'well', 'domains', 'asked', 'receive', 'e', 'mail', 'evaluation', 'number', 'number', 'optional', 'one', 'marketing', 'specialists', 'evaluate', 'sales', 'letter', 'offer', 'expertise', 'make', 'successful', 'standard', 'pricing', 'emails', 'delivered', 'number', 'million', 'number', 'number', 'per', 'number', 'million', 'number', 'number', 'per', 'number', 'million', 'number', 'number', 'per', 'number', 'million', 'number', 'number', 'per', 'special', 'offer', 'introductory', 'offer', 'number', 'number', 'includes', 'number', 'set', 'fee', 'number', 'evaluation', 'sales', 'letter', 'number', 'number', 'number', 'e', 'mails', 'delivered', 'payment', 'policy', 'services', 'must', 'paid', 'full', 'prior', 'delivery', 'advertisement', 'notice', 'absolutely', 'threatening', 'questionable', 'materials', 'serious', 'direct', 'email', 'marketing', 'fax', 'following', 'number', 'number', 'number', 'please', 'fill', 'form', 'completely', 'contact', 'name', '_____________________________________________', 'business', 'name', '______________________________________', 'years', 'business', '_________________________', 'business', 'type', '______________________________________', 'address', '_________________________________________________', 'city', '____________________', 'state', '______', 'zip', '______________', 'country', '_______________', 'email', 'address', '_______________________________________________', 'phone', '__________________________', 'fax', '_______________________', 'toll', 'free', 'phone', 'get', 'email', 'database', 'send', 'email', 'mailto', 'publicservicenumber', 'url'], Hasil=SparseVector(262144, {3336: 1.0, 4978: 1.0, 5078: 2.0, 6122: 2.0, 7441: 1.0, 8145: 1.0, 8538: 1.0, 8804: 1.0, 10706: 1.0, 12650: 4.0, 14376: 3.0, 19684: 1.0, 20575: 1.0, 21622: 1.0, 21823: 3.0, 22400: 1.0, 25262: 1.0, 26144: 2.0, 29526: 1.0, 35119: 2.0, 38333: 1.0, 40402: 1.0, 43756: 1.0, 51471: 1.0, 52879: 1.0, 52894: 1.0, 52907: 1.0, 55307: 4.0, 56001: 2.0, 60458: 1.0, 61444: 1.0, 61710: 1.0, 62127: 1.0, 62930: 1.0, 63956: 1.0, 65568: 1.0, 67483: 1.0, 67662: 3.0, 69373: 1.0, 70831: 1.0, 72569: 1.0, 73199: 1.0, 73785: 1.0, 74318: 1.0, 76106: 2.0, 80649: 1.0, 82035: 1.0, 82252: 1.0, 82343: 1.0, 84007: 2.0, 85163: 1.0, 86769: 1.0, 89717: 1.0, 89833: 1.0, 92032: 1.0, 94266: 1.0, 97757: 1.0, 100221: 1.0, 100620: 2.0, 105891: 2.0, 107201: 1.0, 107584: 2.0, 107977: 1.0, 109156: 2.0, 110264: 1.0, 110427: 1.0, 110428: 1.0, 113241: 2.0, 113984: 1.0, 115086: 1.0, 116251: 2.0, 116782: 4.0, 118870: 1.0, 122254: 1.0, 124217: 1.0, 126634: 1.0, 127041: 1.0, 131091: 2.0, 132975: 1.0, 133555: 1.0, 135239: 27.0, 138021: 1.0, 138201: 1.0, 138313: 1.0, 140931: 2.0, 149982: 1.0, 151144: 1.0, 151393: 1.0, 152164: 2.0, 163551: 1.0, 163674: 2.0, 163886: 1.0, 166368: 1.0, 167503: 1.0, 167694: 9.0, 167721: 1.0, 168380: 3.0, 168546: 1.0, 171734: 1.0, 173565: 1.0, 173681: 2.0, 175799: 1.0, 178005: 1.0, 179995: 1.0, 180100: 1.0, 180270: 2.0, 180547: 1.0, 181321: 1.0, 181750: 2.0, 185620: 1.0, 186436: 3.0, 186925: 2.0, 186972: 1.0, 189904: 1.0, 190256: 1.0, 190608: 1.0, 190861: 1.0, 190922: 1.0, 194240: 1.0, 194618: 1.0, 194882: 5.0, 196946: 1.0, 197785: 1.0, 198131: 8.0, 198132: 3.0, 199468: 1.0, 200728: 1.0, 201183: 1.0, 205397: 1.0, 205538: 1.0, 207150: 1.0, 209518: 1.0, 210897: 2.0, 211302: 1.0, 212415: 1.0, 213766: 5.0, 215686: 1.0, 216008: 1.0, 219195: 1.0, 224692: 1.0, 227040: 1.0, 229407: 1.0, 229463: 1.0, 229543: 1.0, 230101: 1.0, 233575: 1.0, 234128: 1.0, 234943: 1.0, 236461: 1.0, 237377: 1.0, 237761: 1.0, 238047: 1.0, 238167: 1.0, 238830: 1.0, 239713: 1.0, 241692: 1.0, 243875: 1.0, 247372: 2.0, 248200: 2.0, 251606: 7.0, 252722: 1.0, 253913: 1.0, 257091: 2.0, 260160: 1.0}))]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stemming\n",
    "tokenizedTes = tokenizer.transform(TeData)\n",
    "swRemovedTest = stopWord.transform(tokenizedTes)\n",
    "tesAngka = hashingTF.transform(swRemovedTest).select('label', 'Kata Berarti', 'Hasil')\n",
    "# tesAngka.show(5)\n",
    "tesAngka.tail(num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7403fe94-7c0e-4d1e-8c5a-ca68999e05fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3906.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 282.0 failed 1 times, most recent failure: Lost task 0.0 in stage 282.0 (TID 355) (LAPTOP-CKIH6PDK executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (Tokenizer$$Lambda$3630/0x0000000801fab280: (string) => array<string>).\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.lang.NullPointerException\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (Tokenizer$$Lambda$3630/0x0000000801fab280: (string) => array<string>).\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.lang.NullPointerException\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[129], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m predictionFinal \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKata Berarti\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m predictionFinal\u001b[38;5;241m.\u001b[39mtail(num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m correctPrediction \u001b[38;5;241m=\u001b[39m \u001b[43mpredictionFinal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictionFinal\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpredictionFinal\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m totalData \u001b[38;5;241m=\u001b[39m predictionFinal\u001b[38;5;241m.\u001b[39mcount()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect prediction:\u001b[39m\u001b[38;5;124m\"\u001b[39m, correctPrediction, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, total data:\u001b[39m\u001b[38;5;124m\"\u001b[39m, totalData, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, correctPrediction\u001b[38;5;241m/\u001b[39mtotalData)\n",
      "File \u001b[1;32mC:\\spark\\spark\\python\\pyspark\\sql\\dataframe.py:1193\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   1172\u001b[0m \n\u001b[0;32m   1173\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\spark\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\spark\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3906.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 282.0 failed 1 times, most recent failure: Lost task 0.0 in stage 282.0 (TID 355) (LAPTOP-CKIH6PDK executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (Tokenizer$$Lambda$3630/0x0000000801fab280: (string) => array<string>).\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.lang.NullPointerException\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (Tokenizer$$Lambda$3630/0x0000000801fab280: (string) => array<string>).\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.lang.NullPointerException\r\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(tesAngka)\n",
    "predictionFinal = prediction.select(\"Kata Berarti\", \"prediction\", \"label\")\n",
    "predictionFinal.tail(num=2)\n",
    "correctPrediction = predictionFinal.filter(predictionFinal['prediction'] == predictionFinal['label']).count()\n",
    "totalData = predictionFinal.count()\n",
    "print(\"correct prediction:\", correctPrediction, \", total data:\", totalData, \", accuracy:\", correctPrediction/totalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c53759-bfd5-4a5f-aeb0-d9b1357bad31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
